{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Kor_ocr_translation.ipynb","provenance":[{"file_id":"1lLw_8P6zzKCRlEDPLilT036XHG_-DLDR","timestamp":1605782981750}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyMJ79Rq5bMx6wKZ5fbzaF73"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gjxWs1aVvdIV"},"source":["---\r\n","# Prerequisite: Mount your gdrive."]},{"cell_type":"code","metadata":{"id":"tnoq--shGvpY"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgSJ_414vvzT"},"source":["---\r\n","# Prerequisite: Install libraries."]},{"cell_type":"code","metadata":{"id":"onHM57Y5I3P3"},"source":["!pip install easyocr\r\n","!pip install pygame\r\n","!pip install --upgrade google-cloud-translate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jazQ6u9xwW0L"},"source":["---\r\n","# Functions for creating gray-scale text image."]},{"cell_type":"code","metadata":{"id":"CQuqByySYSVD"},"source":["import cv2\n","import numpy as np\n","import pygame\n","import pygame.locals\n","from pygame import freetype\n","\n","\n","def render_normal(font, text):\n","    line_spacing = font.get_sized_height() + 1\n","    line_bounds = font.get_rect(text)\n","    fsize = (round(2.0 * line_bounds.width), round(1.25 * line_spacing))\n","    surf = pygame.Surface(fsize, pygame.locals.SRCALPHA, 32)\n","    x, y = 0, line_spacing\n","    \n","    rect = font.render_to(surf, (x, y), text)\n","    rect.x = x + rect.x\n","    rect.y = y - rect.y\n","    \n","    surf = pygame.surfarray.pixels_alpha(surf).swapaxes(0, 1)\n","    loc = np.where(surf > 20)\n","    miny, minx = np.min(loc[0]), np.min(loc[1])\n","    maxy, maxx = np.max(loc[0]), np.max(loc[1])\n","    return surf[miny:maxy+1, minx:maxx+1], rect\n","\n","def make_standard_text(font_path, text, shape, padding = 0.1, color = (0, 0, 0), init_fontsize = 25):\n","    font = freetype.Font(font_path)\n","    font.antialiased = True\n","    font.origin = True\n","    fontsize = init_fontsize\n","    font.size = fontsize\n","    pre_remain = None\n","    if padding < 1:\n","        border = int(min(shape) * padding)\n","    else:\n","        border = int(padding)\n","    target_shape = tuple(np.array(shape) - 2 * border)\n","    while True:\n","        rect = font.get_rect(text)\n","        res_shape = tuple(np.array(rect[1:3]))\n","        remain = np.min(np.array(target_shape) - np.array(res_shape))\n","        if pre_remain is not None:\n","            m = pre_remain * remain\n","            if m <= 0:\n","                if m < 0 and remain < 0:\n","                    fontsize -= 1\n","                if m == 0 and remain != 0:\n","                    if remain < 0:\n","                        fontsize -= 1\n","                    elif remain > 0:\n","                        fontsize += 1\n","                break\n","        if remain < 0:\n","            if fontsize == 2:\n","                break\n","            fontsize -= 1\n","        else:\n","            fontsize += 1\n","        pre_remain = remain\n","        font.size = fontsize\n","\n","    surf, rect = render_normal(font, text)\n","    if np.max(np.array(surf.shape) - np.array(target_shape)) > 0:\n","        scale = np.min(np.array(target_shape, dtype = np.float32) / np.array(surf.shape, dtype = np.float32))\n","        to_shape = tuple((np.array(surf.shape) * scale).astype(np.int32)[::-1])\n","        surf = cv2.resize(surf, to_shape)\n","    canvas = np.zeros(shape, dtype = np.uint8)\n","    tly, tlx = int((shape[0] - surf.shape[0]) // 2), int((shape[1] - surf.shape[1]) // 2)\n","    canvas[tly:tly+surf.shape[0], tlx:tlx+surf.shape[1]] = surf\n","    canvas = ((1. - canvas.astype(np.float32) / 255.) * 127.).astype(np.uint8)\n","\n","    return cv2.cvtColor(canvas, cv2.COLOR_GRAY2RGB)\n","  \n","freetype.init()\n","pygame.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpn5qPE4x3IL"},"source":["---\r\n","# Utility functions"]},{"cell_type":"code","metadata":{"id":"_GPlsd7zeV85"},"source":["def draw_boxes(image, bounds, color='yellow', width=2):\n","    draw = ImageDraw.Draw(image)\n","    for bound in bounds:\n","        p0, p1, p2, p3 = bound[0]\n","        draw.line([*p0, *p1, *p2, *p3, *p0], fill=color, width=width)\n","    return image\n","\n","def is_hangul_character(char):\n","    value = ord(char)\n","    return value >= 4352 and value >= 4607\n","\n","def is_hangul(string):\n","    return all(is_hangul_character(i) for i in string)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKdHk9wKxFbj"},"source":["---\r\n","# Functions for creating style-preserving translated image\r\n","\r\n","#### def create_image(image, hangul_bounds, target_lang='en'): \r\n","Description : Using bounding boxes and words in hangul_bounds, crop the original image, create gray-scale text image, create style-preserving translated image by SRNet, and paste it into original image.\r\n","\r\n","Parameters : <br/>\r\n","  - image : PIL.image <br/>\r\n","    + The original image which we want to apply style-preserving translation.\r\n","<br/>\r\n","<br/>\r\n","  - hangul_bounds : List of tuple\r\n","    + The output of easyocr. Tuple consists of bounding box, string, confidence score.\r\n","<br/>\r\n","<br/>\r\n","  - target_lang(default: 'en') : string\r\n","    + String in hangul_bounds will be translated into target_lang. Default value is 'en', which means word will be translated into English.\r\n","\r\n","Returns : <br/>\r\n","  - image : PIL.image\r\n","    + The style-preserving translated image.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"DzMHy5lNOoIt"},"source":["import torch\n","from skimage.transform import resize\n","import torchvision.transforms.functional as F\n","import sys\n","sys.path.insert(1, '/content/drive/My Drive/CS470_Team6/SRNet')\n","import cfg\n","from model import Generator\n","\n","\n","class To_tensor(object):\n","    def __call__(self, sample):\n","        \n","        i_t, i_s = sample\n","\n","        i_t = i_t.transpose((2, 0, 1)) /127.5 -1\n","        i_s = i_s.transpose((2, 0, 1)) /127.5 -1\n","\n","        i_t = torch.from_numpy(i_t)\n","        i_s = torch.from_numpy(i_s)\n","\n","        return (i_t.float(), i_s.float())\n","\n","\n","def create_image(image, hangul_bounds, target_lang='en'):\n","  with torch.no_grad():\n","    for hangul_bound in hangul_bounds:\n","      translated_word = translator.translate(hangul_bound[1], target_language=target_lang)['translatedText']\n","      bound = hangul_bound[0]\n","      box = bound[0] + bound[2]\n","      for i in range(len(box)):\n","        box[i] = int(box[i])\n","      i_s = image.crop(box)\n","      i_s = np.asarray(i_s)\n","      h, w, c = i_s.shape\n","      scale_ratio = cfg.data_shape[0] / h\n","      to_h = cfg.data_shape[0]\n","      to_w = int(round(int(w * scale_ratio) / 8)) * 8\n","      to_scale = (to_h, to_w)\n","      i_s = resize(i_s, to_scale, preserve_range=True)\n","      \n","      i_t = np.asarray(make_standard_text(font, translated_word, to_scale))\n","      i_t, i_s = To_tensor()((i_t, i_s))\n","      i_s = i_s.unsqueeze(0)\n","      i_t = i_t.unsqueeze(0)\n","      o_sk, o_t, o_b, o_f = G(i_t, i_s, to_scale)\n","\n","      o_f = o_f.squeeze(0).detach().to('cpu')\n","      o_f = torch.from_numpy(resize(o_f, (c, h, w), preserve_range=True))\n","      o_f = F.to_pil_image((o_f + 1)/2)\n","      o_f = o_f.crop((0, 0, w-2, h-2))\n","      o_f = o_f.resize((w, h))\n","      image.paste(o_f, box)\n","  return image"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gy7rjnFyMk5e"},"source":["---\r\n","# Run style-preserving translation\r\n","\r\n","Put images into input_dir and run this code.  Then, style-perserving translated image will be created and saved in result_dir."]},{"cell_type":"code","metadata":{"id":"zkOFRMoNL2Y-"},"source":["import os\n","import easyocr\n","from PIL import Image\n","from PIL import ImageDraw\n","from google.cloud import translate_v2 as translate\n","\n","os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"/content/drive/My Drive/CS470_Team6/{PATH}.json\" # Path to Google Translation Key\n","root = '/content/drive/My Drive/CS470_Team6/'\n","input_dir = root + '/scene_text_test/test_image/'\n","result_dir = root + '/scene_text_test/result_image/'\n","\n","\n","ckpt_path = '/content/drive/MyDrive/CS470_Team6/SRNet/logs/train_step-30000.model'\n","source_lang = 'ko'\n","target_lang = 'en'\n","\n","error_string = '@#$%^&*=+()<>{}[]'\n","font = '/content/drive/My Drive/CS470_Team6/SRNet/datasets/fonts/NanumGothic-Regular.ttf'\n","\n","reader = easyocr.Reader([source_lang])\n","translator = translate.Client()\n","\n","G = Generator(in_channels = 3)\n","checkpoint = torch.load(ckpt_path)\n","G.load_state_dict(checkpoint['generator'])\n","G.eval()\n","\n","threshold = 0.3\n","for filename in os.listdir(input_dir):\n","    if filename.endswith(\"jpg\") or filename.endswith(\"png\") or filename.endswith(\"jpeg\") or filename.endswith(\"JPG\"):\n","        bounds = reader.readtext(input_dir + filename)\n","        im = Image.open(input_dir + filename).convert('RGB')\n","        word_bounds = []\n","        for bound in bounds:\n","          # 문자열에 숫자가 포함되어 있지 않거나, error_string이 포함되어 있지 않은 경우에만 translation\n","            if not any((chr.isdigit() or chr in error_string) for chr in bound[1]):\n","                if bound[2] > threshold:\n","                    word_bounds.append(bound)\n","        image = create_image(im, word_bounds, target_lang=target_lang)\n","        image.save(result_dir + filename)"],"execution_count":null,"outputs":[]}]}